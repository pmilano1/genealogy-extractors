# Integration: New Modules + Existing CDP Code

## Architecture Overview

The new modules **enhance** the existing CDP code, they don't replace it.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ENHANCED CDP ORCHESTRATOR                     â”‚
â”‚  (orchestration/enhanced_orchestrator.py)                       â”‚
â”‚                                                                  â”‚
â”‚  Inherits from: cdp_orchestrator.py (KEEPS ALL CDP CODE)       â”‚
â”‚  Adds: Extraction, Caching, GraphQL submission                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   1. Check Cache (NEW)                  â”‚
        â”‚      core/cache.py                      â”‚
        â”‚      - 30-day TTL                       â”‚
        â”‚      - Avoid redundant searches         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼ (cache miss)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   2. Execute Base Search (EXISTING)     â”‚
        â”‚      cdp_orchestrator.py                â”‚
        â”‚      - Navigate to URL (CDP)            â”‚
        â”‚      - Take snapshot (CDP)              â”‚
        â”‚      - Rate limiting                    â”‚
        â”‚      - Check if results found           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   3. Source Module (EXISTING)           â”‚
        â”‚      sources/{source}_cdp.py            â”‚
        â”‚      - build_url()                      â”‚
        â”‚      - check_results()                  â”‚
        â”‚      Returns: found=True/False          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼ (if found)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   4. Extract Records (NEW)              â”‚
        â”‚      extraction/{source}_extractor.py   â”‚
        â”‚      - Parse HTML                       â”‚
        â”‚      - Extract structured data          â”‚
        â”‚      - Calculate match scores           â”‚
        â”‚      Returns: [{name, year, place}...]  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   5. Submit to API (NEW)                â”‚
        â”‚      core/graphql_client.py             â”‚
        â”‚      - submitResearch mutation          â”‚
        â”‚      - logSearchAttempt mutation        â”‚
        â”‚      Submits: Structured records        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   6. Cache Result (NEW)                 â”‚
        â”‚      core/cache.py                      â”‚
        â”‚      - Save for 30 days                 â”‚
        â”‚      - Resume after crash               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## What We Keep (Existing CDP Code)

### `cdp_orchestrator.py` âœ…
- Browser navigation using Chrome DevTools MCP
- Snapshot capture
- Rate limiting
- Error handling
- Parallel search coordination

### `sources/{source}_cdp.py` âœ…
- URL building logic
- Result detection (found/not found)
- Source-specific search parameters

### `cdp_client.py` âœ…
- Chrome DevTools MCP integration
- Page navigation
- Content fetching

### `rate_limiter.py` âœ…
- Rate limiting per source
- Prevents API throttling

## What We Add (New Modules)

### `extraction/{source}_extractor.py` âœ¨
- **Purpose**: Parse HTML â†’ structured records
- **Input**: HTML content from CDP snapshot
- **Output**: `[{name, birth_year, birth_place, url, match_score}, ...]`
- **Fallback**: If parser fails â†’ return URL-only record

### `core/cache.py` âœ¨
- **Purpose**: Avoid redundant searches
- **TTL**: 30 days
- **Benefit**: Resume after crash, reuse across runs

### `core/graphql_client.py` âœ¨
- **Purpose**: Submit structured records to API
- **Mutations**: `submitResearch`, `logSearchAttempt`
- **Benefit**: User reviews 10-20 records, not 6,938

### `core/progress.py` âœ¨
- **Purpose**: Show progress and ETA
- **Benefit**: User knows how long search will take

## Example: Find A Grave Search

### OLD FLOW
```python
orchestrator = CDPOrchestrator()
source = FindAGraveCDPSource()

result = orchestrator.search_source(
    source, "Smith", "John", "London", 1850, 1900
)
# Returns: {'found': True, 'message': 'FOUND (6,938 results)', 'url': '...'}
# User manually reviews 6,938 results
```

### NEW FLOW
```python
orchestrator = EnhancedCDPOrchestrator()
source = FindAGraveCDPSource()

result = orchestrator.search_source_with_extraction(
    source, person_id="123", surname="Smith", given_name="John", 
    location="London", year_min=1850, year_max=1900
)
# Returns: {
#   'found': True, 
#   'message': 'FOUND (6,938 results)',
#   'records': [
#     {'name': 'John Smith', 'birth_year': 1875, 'birth_place': 'London', 'match_score': 85},
#     {'name': 'John Smith', 'birth_year': 1878, 'birth_place': 'London', 'match_score': 82},
#     ...  # 10-20 records total
#   ],
#   'api_response': {...}  # Submitted to GraphQL API
# }
# User reviews 10-20 records in UI
```

## Key Points

1. **All CDP code is preserved** - We inherit from `CDPOrchestrator`
2. **Extractors are optional** - If no extractor exists, falls back to old behavior
3. **Graceful degradation** - If parser fails, returns URL-only record
4. **Backward compatible** - Old scripts still work
5. **Incremental migration** - Can add extractors one source at a time

## Migration Path

1. âœ… **Phase 1**: Build new modules (cache, extractors, GraphQL)
2. ğŸ”„ **Phase 2**: Create `EnhancedCDPOrchestrator` (inherits from old)
3. â³ **Phase 3**: Update scripts to use enhanced orchestrator
4. â³ **Phase 4**: Test with real searches
5. â³ **Phase 5**: Archive old files once validated

**Current Status**: Phase 2 complete, ready for Phase 3

